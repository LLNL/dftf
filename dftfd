#!/usr/bin/python3.8

# Copyright 2023 Lawrence Livermore National Security, LLC and other
# DFTF Project Developers. See the top-level LICENSE file for details.
#
# SPDX-License-Identifier: BSD-3-Clause

import argparse
import configparser
import dateutil
import dateutil.parser
import functools
from http.server import BaseHTTPRequestHandler, ThreadingHTTPServer
import itertools
import json
import logging
from math import ceil, floor
import multiprocessing
import os
import platform
import pprint
import random
import redfish
import redfish_utilities
import subprocess
import sys
import time
import queue
from confluent_kafka import Producer
from confluent_kafka.serialization import StringSerializer, SerializationContext, MessageField
from confluent_kafka.schema_registry import SchemaRegistryClient
from confluent_kafka.schema_registry.avro import AvroSerializer
from confluent_kafka.schema_registry import record_subject_name_strategy

CLOCK_SKEW_LIMIT_SEC=300

DEBUG2=logging.DEBUG-1
DEBUG3=logging.DEBUG-2
logging.addLevelName(DEBUG2, 'DEBUG2')
logging.addLevelName(DEBUG3, 'DEBUG3')

verbosity_levels = {
    'critical': logging.CRITICAL,
    'error': logging.ERROR,
    'warning': logging.WARNING,
    'info': logging.INFO,
    'debug': logging.DEBUG,
    'debug2': DEBUG2,
    'debug3': DEBUG3
}

config = None
logger = None
hostname = platform.node()
cluster_name = hostname.rstrip('0123456789')
log_alerts_file = None

metrics_schema = """{
    "name": "RedfishCrayOemSensors",
    "type": "record",
    "fields": [
        {
            "name": "timestamp",
            "type": {
                "type": "long",
                "logicalType": "timestamp-millis"
            }
        },
        {
            "name": "Location",
            "type": "string"
        },
        {
            "name": "Index",
            "type": "int"
        },
        {
            "name": "ParentalContext",
            "type": "string"
        },
        {
            "name": "ParentalIndex",
            "type": "int"
        },
        {
            "name": "PhysicalContext",
            "type": "string"
        },
        {
            "name": "PhysicalSubContext",
            "type": "string"
        },
        {
            "name": "DeviceSpecificContext",
            "type": "string"
        },
        {
            "name": "EventName",
            "type": "string"
        },
        {
            "name": "Value",
            "type": "double"
        },
        {
            "name": "SensorName",
            "type": "string",
            "default": ""
        },
        {
            "name": "cluster",
            "type": "string",
            "default": ""
        }
    ]
}
"""

events_schema = """{
    "name": "RedfishCrayEvents",
    "type": "record",
    "fields": [
        {
            "name": "timestamp",
            "type": {
                "type": "long",
                "logicalType": "timestamp-millis"
            }
        },
        {
            "name": "Location",
            "type": "string"
        },
        {
            "name": "MessageId",
            "type": "string"
        },
        {
            "name": "Severity",
            "type": "string"
        },
        {
            "name": "Message",
            "type": "string"
        },
        {
            "name": "OriginOfCondition",
            "type": "string"
        },
        {
            "name": "syslog_level",
            "type": "string",
            "default": ""
        },
        {
            "name": "cluster",
            "type": "string",
            "default": ""
        }
    ]
}
"""

def get_name(hostname_or_ip):
    if hostname_or_ip not in get_name.cache:
        try:
            hosts = subprocess.run(['getent', 'hosts', hostname_or_ip], stdout=subprocess.PIPE)
            hosts = hosts.stdout.decode("utf-8").split()
            # Index 0 is the IP address, 1+ are hostnames. We prefer a Cray "x" prefixed
            # name, but use index 1 if an x name is not found.
            name = None
            if len(hosts) > 1:
                for n in hosts[1:]:
                    if n[0] == "x":
                        name = n
                        break
                if name is None:
                    name = hosts[1]
            else:
                name = hostname_or_ip
            get_name.cache[hostname_or_ip] = name
        except:
            pass
    name = None
    if hostname_or_ip in get_name.cache:
        logger.log(DEBUG2, f"get_name({hostname_or_ip}) returning {get_name.cache[hostname_or_ip]}")
        name = get_name.cache[hostname_or_ip]
    else:
        logger.log(DEBUG2, f"get_name({hostname_or_ip}) returning {hostname_or_ip} (NOT in cache)")
        name = hostname_or_ip

    return name
get_name.cache = {}

def getent_hosts(hostname_or_ip):
    hosts = None
    try:
        hosts = subprocess.run(['getent', 'hosts', hostname_or_ip], stdout=subprocess.PIPE)
        hosts = hosts.stdout.decode("utf-8").split()
    except:
        raise

    return hosts

def find_and_purge_subscriptions(session,
                                 subscriptions,
                                 destination,
                                 contexts):
    """Returns a list of contexts for subscriptions that are found.

       As it works, any subscriptions that have our destination, but the wrong context,
       will be removed.
    """
    found_contexts = []

    for sub in subscriptions:
        if sub['Destination'] != destination:
            continue
        if 'Context' not in sub or sub['Context'] not in contexts:
            logger.warning(f'purging subscription {sub}')
            session.delete(sub['@odata.id'])
            continue
        found_contexts.append(sub['Context'])

    logger.debug(f'found existing subscriptions {found_contexts}')
    return found_contexts

def update_subscriptions(host, destination):
    """Update subscriptions for 'host'

    Check what, if any, subscriptions exist for 'host', and only add
    subscriptions that are missing."""

    sub1_exists = False
    sub2_exists = False

    try:
        session = redfish.redfish_client(base_url="https://"+host,
                                         username=config.redfish_username,
                                         password=config.redfish_password,
                                         default_prefix="/redfish/v1",
                                         timeout=config.subscription_timeout,
                                         max_retry=config.subscription_retries)
        session.login(auth="session")
    except:
        return (sub1_exists, sub2_exists)

    try:
        subscriptions = redfish_utilities.get_event_subscriptions(session)
        contexts = {"telemetry": "dftfd_CrayTelemetry",
                    "all": "dftfd_all"}
        found = find_and_purge_subscriptions(session, subscriptions,
                                             destination, contexts.values())

        if contexts["telemetry"] in found:
            sub1_exists = True
        elif config.kafka:
            response = redfish_utilities.create_event_subscription(
                context=session,
                destination=destination,
                client_context=contexts["telemetry"],
                registries=["CrayTelemetry"])
            if response:
                sub1_exists = True

        if contexts["all"] in found:
            sub2_exists = True
        else:
            response = redfish_utilities.create_event_subscription(
                context=session,
                destination=destination,
                client_context=contexts["all"],
                registries=[])
            if response:
                sub2_exists = True
    except:
        return (sub1_exists, sub2_exists)
    finally:
        try:
            session.logout()
        except:
            pass

    return (sub1_exists, sub2_exists)

class Handle_HTTP_Connection(BaseHTTPRequestHandler):
    def __init__(self, worker_queues, worker_mapping, sample_period,
                 *args, **kwargs):
        self.worker_queue = worker_queue
        self.worker_mapping = worker_mapping
        self.sample_period = sample_period
        super().__init__(*args, **kwargs)

    def log_request(self, code='-', size='-'):
        # silence internal library messages for each request
        return

    def do_GET(self):
        logger.warning(f'Invalid GET request, Path: {str(self.path)}, Headers:{str(self.headers)}')
        self.send_response_only(405)

    def do_POST(self):
        client_ip = self.client_address[0]
        content_length = int(self.headers['Content-Length'])
        post_data = self.rfile.read(content_length)

        logger.debug(f'queueing POSTed content from {client_ip}, content length = {content_length}')
        try:
            idx = worker_mapping[client_ip]
        except:
            logger.warning(f'IP {client_ip} does not have a preassigned worker, perhaps IP lookup failed?')
            idx = random.randrange(0, len(worker_queues))
        self.worker_queue[idx].put((self.path, self.headers, client_ip, post_data))

        self.send_response(200)
        self.send_header('Content-type', 'text/html')
        self.end_headers()
        self.wfile.write('<html><body><p>OK</p></body></html>'.encode('utf-8'))

def run_http_server(worker_queues, ip_address, port):
    # Assign redfish server IP to fixed worker processes
    ips = []
    for host in config.hosts:
        try:
            ips.append(getent_hosts(host)[0])
        except:
            logger.warning(f'failed to look up IP of {host} using "getent hosts"')
    num_workers = len(worker_queues)
    num_ips = len(ips)
    indices = sorted(list(range(num_workers)) * ceil(num_ips/num_workers))
    random.shuffle(indices)
    # worker_mapping is a dict, the keys are IPs of incoming redfish
    # servers, and the values are indices into the worker_queues array.
    worker_mapping = dict(zip(ips, indices))

    server_address = (ip_address, int(port))
    handler = functools.partial(Handle_HTTP_Connection,
                                worker_queues,
                                worker_mapping,
                                config.sample_period)
    httpd = ThreadingHTTPServer(server_address, handler)
    logger.debug(f'Starting http server on port {port}')
    try:
        httpd.serve_forever()
    except KeyboardInterrupt:
        pass
    logger.warning('Stopping http server')
    httpd.server_close()

def removeprefix(string_val, prefix):
    '''The string class has a removeprefix() method in 3.9+'''
    if string_val[:len(prefix)] == prefix:
        return string_val[len(prefix):]
    else:
        return string_val

def get_sensors_CrayTelemetry(event):
    eventname = removeprefix(event['MessageId'], 'CrayTelemetry.')
    sensor_ignored_count = 0
    for sensor in event['Oem']['Sensors']:
        if 'Location' not in sensor:
            logger.warning('sensor is missing Location')
            sensor_ignored_count += 1
            continue
        if 'Timestamp' not in sensor:
            logger.warning(f'sensor from {sensor["Location"]} is missing Timestamp')
            sensor_ignored_count += 1
            continue
        sensor['timestamp'] = parse_timestamp(sensor['Timestamp'], sensor['Location'])
        if 'Value' not in sensor:
            logger.warning(f'sensor from {sensor["Location"]} is missing Value')
            sensor_ignored_count += 1
            continue

        # The order in which we check the individual fields and append them to
        # "name" is important. We are trying to make it match the order found
        # in the Sensors string of the Redfish endpoint. While we don't work
        # with the endpoints here, keeping the names the same will help humans
        # tie the CrayTelemetry events to their associated Redfish enpoints.
        # The sensor URI looks like:
        #   ${hostname}/redfish/v1/Chassis/${ChassisId}/Sensors/${SensorId}
        # and the SensorId format is:
        #   [ParentalContext][ParentalIndex][PhysicalContext][Index][DeviceSpecificContext][PhysicalSubContext][ReadingType]
        # where "ReadingType" is the EventName derived from the MessageId.
        name = []
        try:
            name.append(sensor['ParentalContext'])
        except KeyError:
            sensor['ParentalContext'] = ''
        try:
            name.append(str(sensor['ParentalIndex']))
        except KeyError:
            sensor['ParentalIndex'] = -1
        try:
            name.append(sensor['PhysicalContext'])
        except KeyError:
            sensor['PhysicalContext'] = ''
        try:
            name.append(str(sensor['Index']))
        except KeyError:
            sensor['Index'] = -1
        try:
            name.append(sensor['DeviceSpecificContext'])
        except KeyError:
            sensor['DeviceSpecificContext'] = ''
        try:
            name.append(sensor['PhysicalSubContext'])
        except KeyError:
            sensor['PhysicalSubContext'] = ''
        sensor['EventName'] = eventname
        name.append(sensor['EventName'])

        sensor_name = ''.join(name)
        sensor['SensorName'] = sensor_name
        sensor['cluster'] = cluster_name

        logger.log(DEBUG3, f"{sensor['timestamp']} {sensor['Location']:10} {sensor['SensorName']:45} {sensor['Value']:10}")

        yield (sensor_name, sensor)

def queue_sensor_CrayTelemetry(sensor, producer, metrics_serializer, string_serializer):
    topic = config.topic_prefix+"craytelemetry"
    try:
        producer.produce(topic=topic,
                         value=metrics_serializer(sensor,
                                               SerializationContext(topic, MessageField.VALUE)),
                         on_delivery=delivery_report)
    except KeyboardInterrupt:
        pass
    except ValueError:
        logger.error("invalid input for kafka topic, discarding record")

# returns integers milliseconds-since-epoch
WARNING_LIMIT_MS = 24*60*60*1000
def parse_timestamp(timestamp_string, location):
    now = round(time.time() * 1000)
    try:
        ts_datetime = dateutil.parser.parse(timestamp_string)
    except:
        logger.warning(f"Unable to parse EventTimestamp {timestamp_string} from {location}")
        return now
    ts = round(ts_datetime.timestamp() * 1000)
    skew = abs(now - ts)
    if skew > (CLOCK_SKEW_LIMIT_SEC*1000):
        if location not in parse_timestamp.last_warning \
           or ((now - parse_timestamp.last_warning[location]) > WARNING_LIMIT_MS):
            logger.warning(f'Excessive clock skew from {location}: {skew//1000}s')
        parse_timestamp.last_warning[location] = now
        ts = now
    return ts
parse_timestamp.last_warning = {}

redfish_to_syslog_severity = {
    "OK": "information",
    "Warning": "warning",
    "Critical": "error"
}
def handle_event_normal(event, producer, events_serializer, string_serializer, client_ip):
    # client_ip is the IP address as a string
    client_hostname = get_name(client_ip)
    if 'EventTimestamp' not in event:
        logger.warning(f'event from {client_hostname} is missing EventTimestamp')
        return
    event['timestamp'] = parse_timestamp(event['EventTimestamp'], client_hostname)
    event['Location'] = client_hostname
    event['cluster'] = cluster_name
    if 'Message' not in event:
        event['Message'] = ''
    if 'MessageId' not in event:
        event['MessageId'] = ''
    if 'Severity' not in event:
        event['Severity'] = ''
        event['syslog_level'] = 'unknown'
    elif event['Severity'] in redfish_to_syslog_severity:
        event['syslog_level'] = redfish_to_syslog_severity[event['Severity']]
    else:
        event['syslog_level'] = 'unknown'
    if 'OriginOfCondition' not in event or '@odata.id' not in event['OriginOfCondition']:
        event['OriginOfCondition'] = ''
    else:
        event['OriginOfCondition'] = event['OriginOfCondition']['@odata.id']
    if log_alerts_file:
        log_alerts_file.write(f"{timestamp} {event['Location']} {event['MessageId']} {event['Severity']} \"{event['Message']}\" {event['OriginOfCondition']}\n")
        log_alerts_file.flush()
    if config.log_alerts:
        logger.log(logging.INFO, f"{timestamp} {event['Location']} {event['MessageId']} {event['Severity']} \"{event['Message']}\" {event['OriginOfCondition']}")
    else:
        logger.log(DEBUG3, f"{event['timestamp']} {event['Location']} {event['MessageId']} {event['Severity']} {event['Message']} {event['OriginOfCondition']}")

    if config.kafka:
        topic = config.topic_prefix+"crayevents"
        try:
            producer.produce(topic=topic,
                             value=events_serializer(event,
                                                     SerializationContext(topic, MessageField.VALUE)),
                             on_delivery=delivery_report)
        except KeyboardInterrupt:
            pass
        except ValueError:
            logger.error("invalid input for kafka topic, discarding record")

def delivery_report(err, msg):
    """
    Reports the failure or success of a message delivery.

    Args:
        err (KafkaError): The error that occurred on None on success.

        msg (Message): The message that was produced or failed.

    Note:
        In the delivery report callback the Message.key() and Message.value()
        will be the binary format as encoded by any configured Serializers and
        not the same object that was passed to produce().
        If you wish to pass the original object(s) for key and value to delivery
        report callback we recommend a bound callback or lambda where you pass
        the objects along.
    """

    if err is not None:
        logger.error(f'Delivery failed for record: {err}')
        return
    logger.log(DEBUG3, f'record successfully produced to {msg.topic()} [{msg.partition()}] at offset {msg.offset()}')

def event_is(event, message_id_prefix):
    prefix_len = len(message_id_prefix)
    if event['MessageId'][:prefix_len] == message_id_prefix:
        return True
    else:
        return False

def worker_process(worker_queue, kafka_conf, schema_registry_conf):
    logger.debug('start')

    producer = None
    metrics_serializer = None
    string_serializer = None
    events_serializer = None
    if config.kafka:
        schema_registry_client = SchemaRegistryClient(schema_registry_conf)
        avro_serializer_conf = {}
        avro_serializer_conf['subject.name.strategy'] = record_subject_name_strategy
        metrics_serializer = AvroSerializer(schema_registry_client,
                                            metrics_schema,
                                            to_dict=None,
                                            conf=avro_serializer_conf)
        string_serializer = StringSerializer('utf_8')
        events_serializer = AvroSerializer(schema_registry_client,
                                           events_schema,
                                           to_dict=None,
                                           conf=avro_serializer_conf)
        producer = Producer(kafka_conf)

    client_last_accepted = {}
    while True:
        try:
            work = worker_queue.get()
        except:
            if config.kafka:
                producer.flush()
            return
        if config.kafka:
            producer.poll(0.0)
        if work is None:
            # None signals time to exit, put it back on the queue
            # so the next worker will exit as well.
            logger.debug('end (received "None" exit message)')
            if config.kafka:
                producer.flush()
            worker_queue.put(None)
            break

        # decode the message
        logger.debug(f'start handling http POST')
        ts = time.time()
        try:
            (path, headers, client_ip, data) = work
        except:
            logger.error(f'Unable to unpack "work" tuple')
            continue
        try:
            data_decoded = data.decode('utf-8')
        except:
            logger.error(f'Unable to decode("utf-8") the data from {client_ip}')
            continue
        try:
            json_data = json.loads(data_decoded)
        except:
            logger.error(f'Failed JSON decode of data from {client_ip}, data starts with: "{data_decoded[:40]}"')
            continue

        # Filter out CrayTelemetry if config.sample_period has not expired
        is_cray_telemetry = False
        for event in json_data['Events']:
            if event_is(event, 'CrayTelemetry'):
                is_cray_telemetry = True
                break
        if is_cray_telemetry:
            if client_ip not in client_last_accepted:
                client_last_accepted[client_ip] = 0
            delta = ts - client_last_accepted[client_ip]
            if delta < config.sample_period:
                # Discard the CrayTelemtry events
                logger.debug(f'skipping CrayTelemetry event from {client_ip}, delta {delta:.2f}s')
                continue
            else:
                logger.debug(f'processing CrayTelemetry event from {client_ip}, delta {delta:.2f}s')
                client_last_accepted[client_ip] = ts
        else:
            logger.debug(f'processing other event from {client_ip}')

        # Publish the remaining events
        metrics_dict = {}
        metrics_avail = 0
        metrics_dropped = 0
        events_count = 0
        for event in json_data['Events']:
            if event_is(event, 'CrayTelemetry'):
                # look through all sensors, and only keep the most
                # recent values for each unique sensor
                for sensor_name, sensor in get_sensors_CrayTelemetry(event):
                    metrics_avail += 1
                    if sensor_name not in metrics_dict:
                        metrics_dict[sensor_name] = sensor
                    elif metrics_dict[sensor_name]['timestamp'] < sensor['timestamp']:
                        metrics_dict[sensor_name] = sensor
                        metrics_dropped += 1
                    else:
                        metrics_dropped += 1
            else:
                handle_event_normal(event, producer, events_serializer, string_serializer, client_ip)
                events_count += 1
        for metric in metrics_dict.values():
            if config.kafka:
                queue_sensor_CrayTelemetry(metric, producer, metrics_serializer, string_serializer)
        if config.kafka:
            producer.poll(0.0)

        if events_count > 0:
            logger.log(DEBUG2, f"events: {events_count}")
        if metrics_avail > 0:
            logger.log(DEBUG2, f"kept metrics: {metrics_avail-metrics_dropped}, dropped metrics: {metrics_dropped}({(metrics_dropped)/metrics_avail*100:.1f}%)")
        logger.debug('end handling http POST')

def subscriber_process(subscriber_queue, ip_address, port):
    destination = ip_address+":"+str(port)
    while True:
        before = floor(time.time())
        logger.debug('start (re)subscription procedure')
        num_pool_processes = min(200, len(config.hosts))
        with multiprocessing.Pool(num_pool_processes) as pool:
            args = zip(config.hosts, itertools.repeat(destination))
            result = pool.starmap_async(update_subscriptions, args)
            while not result.ready():
                # Check for the abort signal from the control process
                try:
                    subscriber_queue.get(block=True, timeout=1)
                except queue.Empty:
                    # no signal, just keep working
                    pass
                else:
                    return
        after = floor(time.time())
        logger.debug(f'end (re)subscription procedure ({after - before}s)')
        sleep_time = config.resubscribe_interval - (after - before)
        if sleep_time < 1:
            logger.warning(f'subscription updates are taking longer than resubscribe_interval of {config.resubscribe_interval}')
            sleep_time = 0
        try:
            subscriber_queue.get(block=True, timeout=sleep_time)
        except queue.Empty:
            # The timeout is just used to wake this process up for the next round of
            # subscription udpates.
            pass
        except KeyboardInterrupt:
            return
        else:
            # Getting _any_ message means it is time to exit
            return

def parse_configuration():
    parent_parser = argparse.ArgumentParser(add_help=False)
    parent_parser.add_argument("-c", "--conf_file",
                               help="configuration file",
                               metavar="FILE")
    args, argv = parent_parser.parse_known_args()

    defaults = {}
    file_argv = []
    hosts = []
    kafka_conf = {}
    schema_registry_conf = {}
    if args.conf_file:
        cp = configparser.ConfigParser(allow_no_value=True)
        cp.read(args.conf_file)
        for key, value in cp.items("general"):
            file_argv.append('--'+key)
            if value:
                file_argv.append(value)
        argv = file_argv + argv

        if "kafka" in cp:
            kafka_conf = dict(cp.items("kafka"))
        if "schema_registry" in cp:
            schema_registry_conf = dict(cp.items("schema_registry"))

        if hostname in cp:
            defaults['hosts'] = list(cp[hostname].keys())
        else:
            defaults['hosts'] = []

    parser = argparse.ArgumentParser(parents=[parent_parser])
    parser.set_defaults(**defaults)
    parser.add_argument("-u", "--redfish_username")
    parser.add_argument("-p", "--redfish_password")
    parser.add_argument("-A", "--address")
    parser.add_argument("-P", "--port", type=int, default=8093)
    parser.add_argument("-t", "--topic_prefix", default="timeseries-redfish-")
    parser.add_argument("--resubscribe_interval", type=int, default=3600)
    parser.add_argument("-w", "--worker_count", type=int, default=16)
    parser.add_argument("-v", "--verbosity", choices=verbosity_levels.keys(), default='warning')
    parser.add_argument("--log_alerts", action='store_true', default=False)
    parser.add_argument("--log_alerts_file", type=str)
    parser.add_argument("--no_kafka", dest='kafka', action='store_false', default=True)
    parser.add_argument("--sample_period", type=int, default=0)
    parser.add_argument("--subscription_timeout", type=int, default=10)
    parser.add_argument("--subscription_retries", type=int, default=1)

    args = parser.parse_args(argv)

    args_dict = vars(args)
    missing = False
    if args.kafka:
        for required in "redfish_username", "redfish_password":
            if required not in args_dict or args_dict[required] is None:
                print(f'"{required}" must be supplied in the configuration file, or on the command line')
                missing = True
    if missing:
        sys.exit(1)

    # Lookup the ip_address of this node on the "switch" VLAN
    if not args.address:
        shasta_name = hostname+'-shasta'
        try:
            args.address = getent_hosts(shasta_name)[0]
        except:
            args.address = shasta_name

    if args.log_alerts_file:
        global log_alerts_file
        try:
            log_alerts_file = open(args.log_alerts_file, "a")
        except:
            print(f'Unable to open file "{args.log_alerts_file}"')
            raise

    return args, kafka_conf, schema_registry_conf

def main():
    global logger
    global config

    config, kafka_conf, schema_registry_conf = parse_configuration()
    logger = multiprocessing.log_to_stderr(level=verbosity_levels[config.verbosity])

    worker_queues = []
    workers = []
    for _ in range(config.worker_count):
        worker_queue = multiprocessing.Queue()
        worker_queues.append(worker_queue)
        worker = multiprocessing.Process(target=worker_process,
                                         args=(worker_queue,
                                               kafka_conf,
                                               schema_registry_conf))
        workers.append(worker)
        worker.start()

    subscriber_queue = multiprocessing.Queue()
    subscriber = multiprocessing.Process(target=subscriber_process,
                                         args=(subscriber_queue, config.address, config.port))
    subscriber.start()

    run_http_server(worker_queues, config.address, port=config.port)

    # Signal exit by putting "None" in the various queues
    subscriber_queue.put(None)
    subscriber_queue.close()
    for worker_queue in worker_queues:
        worker_queue.put(None)
        worker_queue.close()
    for worker in workers:
        worker.join()
    subscriber.join()

if __name__ == '__main__':
    main()
